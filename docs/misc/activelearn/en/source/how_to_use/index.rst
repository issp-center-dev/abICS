.. _sec_basic_usage:

***************************
Basic usage
***************************

.. highlight:: none

Installation of aenet
-----------------------

In abICS, we use aenet to build neural network models.
You can download aenet from http://ann.atomistic.net.
Follow the Installation instructions in the Documentation to install it.
Note that abICS uses ``train.x`` and ``predict.x`` of aenet for training and evaluating neural networks.
For ``train.x``, an MPI parallel version is available, but for ``predict.x``, you need to use a non-MPI executable file (serial).
For this reason, you should also install the serial version under makefiles.

Installation of GNU parallel (optional)
-----------------------------------------
In this tutorial, we will use GNU parallel to run first-principles calculations with Quantum Espresso in parallel.
Therefore, you need to install GNU parallel first.
GNU parallel can be downloaded from https://www.gnu.org/software/parallel/ (on a Mac, it can also be installed directly by homebrew).
There are also tutorials available free of charge online at this site.
After moving to the directory you downloaded and extracted, you can install it into ``$HOME/opt/parallel`` by typing the following command.

::

  $ ./configure --prefix=$HOME/opt/parallel
  $ make && make install

For detailed configuration, please refer to the official manual.

.. _Input file:

Preparation of the input file
---------------------------------

To use abICS, you need to prepare four different input files.

abICS control file (``input.toml``)
++++++++++++++++++++++++++++++++++++++++++++++++++++
This file contains the definition of the lattice structure to be calculated, the control of the entire active learning loop by abICS, and the parameters for the replica exchange Monte Carlo method.
By using the st2abics tool, you can automatically generate the input.toml template from the crystal structure file.

::

  $ cd [example_dir].
  $ st2abics st2abics_MgAl2O4.toml MgAl2O4.vasp > input.toml


In this example, set tha path of the ``[sampling.solver]`` section in the ``input.toml`` to the path of the aenet ``predict.x`` in your environment, and set the exe_command in the ``[trainer]`` section to the commands for running ``generate.x`` and ``train.x``. In addition, you need to set ``ignore_species = ["O"]`` in ``[sampling.solver]`` and ``[trainer]`` to get it to work.

In this section, we will explain the settings for each section of input.toml in more detail. If you want to run the example right now, you can skip it.

(i) ``[sampling]`` section
****************************************************
.. code-block:: toml

    [sampling]
    nreplicas = 15            
    nprocs_per_replica = 1    
    kTstart = 600.0           
    kTend = 2000.0            
    nsteps = 6400 
    RXtrial_frequency = 4
    sample_frequency = 16
    print_frequency = 1
    reload = false

In this section, you can configure settings related to the number of replicas, temperature range, etc. for the Replica Exchange Monte Carlo (RXMC) method (manual reference link).
This time, we will use anet's ``predict.x`` as the energy solver for RXMC calculations. Currently, the mpi version of ``predict.x`` is not supported, so nprocs_per_replica should be 1.

(ii) ``[mlref]`` section
****************************************************
.. code-block:: toml

    [mlref]
    nreplicas = 15
    ndata = 20

In this section, you can set the options for extracting atomic configurations from the RXMC calculation results to evaluate the accuracy of the neural network model and to expand the training data.
Basically, ``nreplicas`` should be the same values as in the ``[sampling]`` section.
``ndata`` specifies how many samples to be extracted as the training dataset of the machine learning model from configurations generated by the RXMC calculation.
Therefore, it should be set to a value less than or equal to the number of configurations generated by the RXMC calculation, ``nsteps/sample_frequency`` in ``[sampling]`` section.

(iii) ``[sampling.solver]`` section
****************************************************
.. code-block:: toml

    [sampling.solver] # Configure the solver used for RXMC calculations
    type = 'aenet'
    path= '~/git/aenet/bin/predict.x-2.0.4-ifort_serial'
    base_input_dir = '. /baseinput'
    perturb = 0.0
    run_scheme = 'subprocess' 
    ignore_species = ["O"]

In this section, you can configure the energy calculator (solver) to be used for RXMC calculations.
In this article, we will use ``aenet`` package to implement a neural network model.
For ``type``, ``perturb``, and ``run_scheme``, if you are using the active learning scheme, do not change the above example.
Set path to the path of aenet's ``predict.x`` in your environment.
The ``base_input_dir``, where the input files corresponding to ``predict.x`` are generated, can be set freely (explained in detail later). 

You can also specify the atomic species to be ignored in the neural network model as ``ignore_species``.
In this example, the sublattice of oxygen always has an occupancy of 1, so oxygens do not affect energy.
In this case, it is more computationally efficient to ignore the existence when training and evaluating the neural network model.

(iv) ``[mlref.solver]`` section
****************************************************

.. code-block:: toml

    [mlref.solver] # Set up a reference ab initio solver.
    type = 'qe'
    base_input_dir = ['./baseinput_ref', './baseinput_ref', './baseinput_ref'] #, './baseinput_ref']
    perturb = 0.05

Set up the solver used to calculate the energy for training data (placement energy).
In this example, Quantum Espresso is used.
The ``base_input_dir`` can be set freely.
The input files for the solver are placed in the set directory (see below).
If multiple directories are set up in a list format, as in this example, calculations using each input are performed in turn.
The second and subsequent calculations will use the structure from the last step of the previous calculation as the initial coordinates.
The energy of the last calculation is then used for training of the neural network model.
For example, one could perform a fast structural optimization in the first input file at the expense of accuracy, and then perform a structural optimization in the second and subsequent input files with a higher accuracy setting.
For another example, in the case of a lattice vector relaxation, the same input can be run multiple times to reset the computational mesh based on a set plane-wave cutoff.

The ``perturb`` is for starting the structural optimization from a structure with broken symmetry by randomly displacing each atom.
In this case, the first calculation starts from the structure in which all atoms for structural relaxation are displaced by 0.05 angstrom in a random direction.

(v) ``[trainer]`` section
****************************************************

.. code-block:: toml

    [trainer] # Configure the model trainer.
    type = 'aenet'
    base_input_dir = '. /aenet_train_input'
    exe_command = ['~/git/aenet/bin/generate.x-2.0.4-ifort_serial', 
                  'srun ~/git/aenet/bin/train.x-2.0.4-ifort_intelmpi']
    ignore_species = ["O"]

Set up a learner to train a placement energy prediction model from training data.
Currently, abICS supports only aenet.
You can freely set the ``base_input_dir``.
In the configured directory, set up the configuration files for the trainer (see below).
In ``exe_command``, specify the paths to ``generate.x`` and ``train.x`` of aenet. For ``train.x``, an MPI parallel version is available, in which case, set the commands for MPI execution (``mpiexec``, ``srun``, etc.) as shown in the example above.

``ignore_species`` specifies the atomic species to be ignored by NN model (see description in (iii) ``[sampling.solver]``).

(vi) ``[config]`` section
****************************************************

.. code-block:: toml

    [config] # Set up information about the crystal lattice and the atoms and vacancies on the lattice.
    unitcell = [[8.1135997772, 0.0000000000000000, 0.0000000000000000],
                [0.0000000000000000, 8.1135997772, 0.0000000000000000],
                [0.0000000000000000, 0.0000000000000000, 8.1135997772]]
    supercell = [1,1,1]

    [[config.base_structure]]
    type = "O"
    coords = [
        [0.237399980, 0.237399980, 0.237399980],
        [0.762599945, 0.762599945, 0.762599945],
        [0.512599945, 0.012600004, 0.737399936],
        [0.487399966, 0.987399936, 0.262599975],
        ... 

``[config]`` section specifies atomic positions to be used in the Monte Carlo sampling.
The ``st2abics`` utility tool can generate this section.


Input files for ab initio solvers
++++++++++++++++++++++++++++++++++++++++++++++++++++

Prepare an input (reference) file according to the first-principles solver to be used.
The following is a list of reference files required by each solver.

VASP
=====

- URL : https://www.vasp.at

- Reference files

  - Prepare INCAR, POTCAR, and KPOINTS files.

    - In POTCAR file, arrange the elements in alphabetical order.
    - The POSCAR file is not required, but may be required depending on the version of the dependent package ``pymatgen``. In that case, please prepare an appropriate file.


Quantum Espresso
==================

- URL : https://www.quantum-espresso.org

- Please use the version 6.2 or higher.

  - So-called old-style XML versions are not available.

- Reference files

  - The reference file name should be ``scf.in``.
  - Only ``scf`` and ``relax`` are supported for ``calculation``.
  - If you are only using :math:`\Gamma` points, you can speed up the calculation by specifying ``kpoints`` as ``Gamma``.

OpenMX
======

- URL : http://www.openmx-square.org

- Please use the version 3.9.

- Reference files

  - The reference file name should be ``base.dat``.


Input file for training using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++

#TODO we'll ask Kasamatsu to add it.

Input file for deployment energy calculation using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#TODO we'll ask Kasamatsu to add it.

Implementation of active learning
-----------------------------------

Generating training data
++++++++++++++++++++++++++++++++++++++++++++++++++++


(i) Generate input files for ab initio calculations
****************************************************

Use ``abics_mlref`` to generate an input file for first-principles calculations, which will be the main source of the training data.
At the first execution, the specified number of atomic arrangements are randomly generated, a separate directory is prepared for each atomic arrangement, and an input file is created in the directory.
At the same time, it also generates a file ``rundirs.txt`` that contains the paths of those directories.
You can use this directory list to automate the execution of first-principles calculation jobs for individual inputs.
In this tutorial, we will introduce a batch execution method using gnu parallel, keeping in mind a shared computer with slurm installed as a scheduler. 

The information in the input file for ``abics_mlref`` is as follows, it reads the information in the ``[mlref.solver]`` section and generates an input file for first-principles calculations.

- ``type`` : Indicates an ab initio solver. You can select 'vasp', 'qe', or 'openmx'.

- ``base_input_dir``: Represents a list of directories that contain input files referenced by the first-principle solver.

- ``perturb``, ``ignore_species``: Same as in the ``[sampling.solver]`` section.

In ``abics_mlref``, the input files in the folder in ``baseinput_dir`` are used to generate the input files for calculation. (This can also be applied to prepare multiple input files if you want to calculate under strict conditions.) When you execute ``abics_mlref``, it outputs an intermediate file and records the number of times ``abics_mlref`` is executed.
By reading it, it reads the input files stored in the corresponding input folder of ``baseinput_dir``.
If the number of executions is greater than the elements in ``baseinput_dir``, it causes an error.

(ii) Performing ab initio calculations
****************************************************

Execute the first-principle calculation based on the input file created in (i).
If you are using gnu parallel, you can easily perform parallel computations by specifying ``rundirs.txt`` with the ``-a`` option.

When running multiple times in ``baseinput_dir``, you need to run ``abics_mlref`` and perform the calculation in (ii) for the number of times.


Creating a neural network potential using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We create neural network potentials by learning the results of ab initio calculations made by abics_train using aenet.
The input information for abics_train is described in the ``[trainer]`` section.
The description of each parameter is as follows.

- ``type``: The trainer to generate the neural network potential (currently only 'aenet')
- ``base_input_dir``: Path of the directory containing the input files that the learner refers to.
- ``exe_command``: list of commands to execute; if you use aenet, you need to specify the path to generate.x and train.x.
- ``ignore_species``: Same as [sampling.solver] section.

Structure estimation using aenet as a solver and Monte Carlo methods
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We run ``abics_sampling`` and use the Monte Carlo method to estimate the structure.
``abics_sampling`` automatically creates directories such as ``MC0`` and ``MC1``.
It is designed with active learning in mind, and has an additional function to retrieve information such as the number of calculations by reading ``ALloop.progress``.
In the solver section, change type to aenet and set path to the path to ``predict.x``.
Also, in ``base_input_dir``, specify the path to the directory where the neural network and input files created in (b) are placed for running ``predict.x``.
After the input file is created, the structure estimation is performed by running ``abics_sampling``.

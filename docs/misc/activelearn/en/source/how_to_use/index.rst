.. _sec_basic_usage:

***************************
Basic usage
***************************

.. highlight:: none

Installation of aenet
-----------------------

In abICS, we use aenet to build neural network models.
You can download aenet from http://ann.atomistic.net.
Follow the Installation instructions in the Documentation to install it.
Note that abICS uses ``train.x`` and ``predict.x`` of aenet for training and evaluating neural networks.
For ``train.x``, an MPI parallel version is available, but for ``predict.x``, you need to use a non-MPI executable file (serial).
For this reason, you should also install the serial version under makefiles.

Installation of GNU parallel (optional)
-----------------------------------------
In this tutorial, we will use GNU parallel to run first-principles calculations with Quantum Espresso in parallel.
Therefore, you need to install GNU parallel first.
GNU parallel can be downloaded from https://www.gnu.org/software/parallel/ (on a Mac, it can also be installed directly by homebrew).
There are also tutorials available free of charge online at this site.
After moving to the directory you downloaded and extracted, you can install it by typing the following command.

::

  $ . /configure && make && make install

For detailed configuration, please refer to the official manual.

.. _Input file:

Preparation of the input file
-----------------------

To use abICS, you need to prepare four different input files.

abICS control file (input.toml)
++++++++++++++++++++++++++++++++++++++++++++++++++++
This file contains the definition of the lattice structure to be calculated, the control of the entire active learning loop by abICS, and the parameters for the replica exchange Monte Carlo method.
By using the st2abics tool, you can automatically generate the input.toml template from the crystal structure file.

::

  $ cd [example_dir].
  $ st2abics st2abics_MgAl2O4.toml MgAl2O4.vasp > input.toml


In this example, set tha path of the ``[solver]`` section in the input.toml to the path of the aenet ``predict.x`` in your environment, and set the exe_command in the ``[trainer]`` section to the commands for running ``generate.x`` and ``train.x``. In addition, you need to set ``ignore_species = ["O"]`` in ``[solver]`` and ``[trainer]`` to get it to work.

In this section, we will explain the settings for each section of input.toml in more detail. If you want to run the example right now, you can skip it.

(i) [replica] section
****************************************************
.. code-block:: toml

    [replica]
    nreplicas = 15            
    nprocs_per_replica = 1    
    kTstart = 600.0           
    kTend = 2000.0            
    nsteps = 6400 
    RXtrial_frequency = 4
    sample_frequency = 16
    print_frequency = 1
    reload = false

In this section, you can configure settings related to the number of replicas, temperature range, etc. for the Replica Exchange Monte Carlo (RXMC) method (manual reference link).
This time, we will use anet's ``predict.x`` as the energy solver for RXMC calculations. Currently, the mpi version of ``predict.x`` is not supported, so nprocs_per_replica should be 1.

(ii) [replicaRef] section
****************************************************
.. code-block:: toml

    [replicaRef]
    nreplicas = 15
    nprocs_per_replica = 1
    nsteps = 400
    sample_frequency = 20

In this section, you can set the options for extracting atomic configurations from the RXMC calculation results to evaluate the accuracy of the neural network model and to expand the training data. Basically, nreplicas and nprocs_per_replica should be the same values as in the [replica] section. nsteps specifies how many steps to take out of the number of placements generated by the RXMC calculation (the value of nsteps/sample_frequency in the [replica] section). Therefore, it should be set to a value less than or equal to the number of placements output by the RXMC calculation. By setting a smaller value, you can focus on the early part of the RXMC calculation (For example, you can focus on extracting the RXMC calculation before it equilibrates). Also, the sample_frequency in the [replicaRef] section specifies the interval at which the deployments are extracted. In the above case, 20 different structures of step 0, 20, 40, ... 380 are extracted for each replica at 20 step intervals. In total, the input files for first-principles calculations for 20 x 15 = 300 configurations are generated.


(iii) [solver] section
****************************************************
.. code-block:: toml

    [solver] # Configure the solver used for RXMC calculations
    type = 'aenet'
    path= '~/git/aenet/bin/predict.x-2.0.4-ifort_serial'
    base_input_dir = '. /baseinput'
    perturb = 0.0
    run_scheme = 'subprocess' 
    ignore_species = ["O"]

In this section, you can configure the energy solver to be used for RXMC calculations. In this article, we will use anet to evaluate a neural network model. For type, perturb, and run_scheme, if you are using the active learning scheme, do not change the above example. Set path to the path of aenet's ``predict.x`` in your environment. The base_input_dir can be set freely. Place the input files corresponding to ``predict.x`` in the directory you set (explained in detail later). 

You can also specify the atomic species to ignore in the neural network model as ignore_species. In this example, the sublattice of O always has an occupancy of 1, so O's placement does not affect energy. In this case, it is more computationally efficient to ignore the existence when training and evaluating the neural network model.

.. code-block:: toml

    [solverRef] # Set up a reference ab initio solver.
    type = 'qe'
    path = '' # ignored for active learning
    base_input_dir = ['./baseinput_ref', './baseinput_ref', './baseinput_ref'] #, './baseinput_ref']
    perturb = 0.05
    run_scheme = 'subprocess'
    only_input = true
    ignore_species = []
    vac_convert = []

    [trainer] # Configure the model trainer.
    type = 'aenet'
    base_input_dir = '. /aenet_train_input'
    exe_command = ['~/git/aenet/bin/generate.x-2.0.4-ifort_serial', 
                  'srun ~/git/aenet/bin/train.x-2.0.4-ifort_intelmpi']
    ignore_species = ["O"]
    vac_map = []
    restart = false

    [config] # Set up information about the crystal lattice and the atoms and vacancies on the lattice.
    unitcell = [[8.1135997772, 0.0000000000000000, 0.0000000000000000],
                [0.0000000000000000, 8.1135997772, 0.0000000000000000],
                [0.0000000000000000, 0.0000000000000000, 8.1135997772]]
    supercell = [1,1,1]

    [[config.base_structure]]
    type = "O"
    coords = [
        [0.237399980, 0.237399980, 0.237399980],
        [0.762599945, 0.762599945, 0.762599945],
        [0.512599945, 0.012600004, 0.737399936],
        [0.487399966, 0.987399936, 0.262599975],
        ... 

Input files for ab initio solvers
++++++++++++++++++++++++++++++++++++++++++++++++++++

Prepare an input (reference) file according to the first-principles solver to be used. The following is a list of reference files required by each solver.

VASP
=====

- URL : https://www.vasp.at

- Reference files

  - Prepare INCAR, POTCAR, and KPOINTS files.

    - In POTCAR file, arrange the elements in alphabetical order.
    - The POSCAR file is not required, but may be required depending on the version of the dependent package ``pymatgen``. In that case, please prepare an appropriate file.


Quantum Espresso
================

- URL : https://www.quantum-espresso.org

- Please use the version 6.2 or higher.

  - So-called old-style XML versions are not available.

- Reference files

  - The reference file name should be ``scf.in``.
  - Only ``scf`` and ``relax`` are supported for ``calculation``.
  - If you are only using :math:`\Gamma` points, you can speed up the calculation by specifying ``kpoints`` as ``Gamma``.

OpenMX
======

- URL : http://www.openmx-square.org

- Please use the version 3.9.

- Reference files

  - The reference file name should be ``base.dat``.


Input file for training using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++

#TODO we'll ask Kasamatsu to add it.

Input file for deployment energy calculation using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++

#TODO we'll ask Kasamatsu to add it.

Implementation of active learning
-----------------------

Generating training data
++++++++++++++++++++++++++++++++++++++++++++++++++++


(i) Generate input files for ab initio calculations
****************************************************

Use abics_activelearn to generate an input file for first-principles calculations, which will be the main source of the training data. At the first execution, the specified number of atomic arrangements are randomly generated, a separate directory is prepared for each atomic arrangement, and an input file is created in the directory. At the same time, it also generates a file rundirs.txt that contains the paths of those directories. You can use this directory list to automate the execution of first-principles calculation jobs for individual inputs. In this tutorial, we will introduce a batch execution method using gnu parallel, keeping in mind a shared computer with slurm installed as a scheduler. 

The information in the input file for abics_activelearn is as follows, it reads the information in the [solverRef] section and generates an input file for first-principles calculations.

- type : Indicates an ab initio solver. You can select 'vasp', 'qe', or 'openmx'.

- path: Specifies the path to the aenet executable file (predict.x).

- base_input_dir: Represents a list of directories that contain input files referenced by the first-principle solver.

- perturb, run_scheme, ignore\_species: Same as in the [solver] section.

In abics\_activelearn, the input files in the folder in baseinput\_dir are used to generate the input files for calculation (This can also be applied to prepare multiple input files if you want to calculate under strict conditions). When you execute abics_activelearn, it outputs an intermediate file and records the number of times abics \ _activelearn is executed.
By reading it, it reads the input files stored in the corresponding input folder of baseinput\_dir. If the number of executions is greater than the elements in baseinput\_dir, it causes an error.

(ii) Performing ab initio calculations
****************************************************

Execute the first-principle calculation based on the input file created in (i).
If you are using gnu parallel, you can easily perform parallel computations by specifying rundirs.txt with the -a option.

When running multiple times in baseinput\_dir, you need to run abics\_activelearn and perform the calculation in (ii) for the number of times.


Creating a neural network potential using aenet
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We create neural network potentials by learning the results of ab initio calculations made by abics_train using aenet.
The input information for abics_train is described in the [trainer] section. The description of each parameter is as follows.

- type: The trainer to generate the neural network potential (currently only 'aenet')
- base_input_dir: Path of the directory containing the input files that the learner refers to.
- exe_command: list of commands to execute; if you use aenet, you need to specify the path to generate.x and train.x.
- ignore_species: Same as [solver] section.

Structure estimation using aenet as a solver and Monte Carlo methods
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We run abICSAL and use the Monte Carlo method to estimate the structure. abICSAL automatically creates directories such as MC0 and MC1.
It is designed with active learning in mind, and has an additional function to retrieve information such as the number of calculations by reading ALloop.progress. In the solver section, change type to aenet and set path to the path to predict.x.
Also, in base_input_dir, specify the path to the directory where the neural network and input files created in (b) are placed for running ``predict.x``. After the input file is created, the structure estimation is performed by running abICSAL.

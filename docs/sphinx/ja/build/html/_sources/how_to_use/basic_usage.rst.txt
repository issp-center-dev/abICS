.. highlight:: none

基本的な使用方法について
----------------------

abICSでは, モンテカルロステップ毎に位置座標を更新しながら、ソルバー(VASP, QEなど)によるエネルギー計算などを行います.
そのため、位置座標以外の情報について予め用意する必要があります。これらの情報は、ソルバーの入力形式に従ったファイルを参照し取得します。


参照ファイルの準備
-----------------------

使用するソルバーの入力形式に従った入力ファイルを用意します。
参照ファイルのパスはabICSの入力ファイルにある ``[solver]`` セクションの ``base_input_dir`` で指定します。
座標情報については、abICSの入力ファイルを参照するため、記載する必要はありません。
以下、QEの参照ファイルの例について記載します。

.. literalinclude::  ../../../../../examples/standard/spinel/baseinput/scf.in

		     
		     
入力ファイルの作成
-----------------------

次にabICSの入力ファイルを作成します。
abICSの入力ファイルは, 以下の４つのセクションから構成されます.

1. [replica] セクション
   レプリカ数や温度の幅, モンテカルロステップ数など,レプリカ交換モンテカルロ部分のパラメータを指定します.

2. [solver] セクション
   ソルバーの種類 (VASP, QE, ...)、ソルバーへのパス、不変な入力ファイルのあるディレクトリなど（第一原理計算）ソルバーのパラメータを指定します.

3. [observer] セクション
   取得する物理量の種類などを指定します.

4. [config] セクション
   合金の配位などを指定します.

これらの詳細については :doc:`../file_specification/index` をご覧ください。
以下、QEの場合の入力ファイルの例を記載します。

.. literalinclude::  ../../../../../examples/standard/spinel/input_qe.toml


abICSの実行
-------------------

MPI 実行時に指定するプロセス数はレプリカ数以上である必要があります。

::

 $ mpiexec -np 2 abics input.toml

実行すると、カレントディレクトリ以下にレプリカ番号を名前にもつディレクトリが作られ、各レプリカはその中でソルバーを実行します。

MPI プロセス数に関する Tips
============================
abICS はソルバーの実行のために ``MPI_Comm_spawn`` という MPI ライブラリ関数を使用します。
この関数は新規に MPI プロセスを(複数)起動して、そこで任意プログラムを(並列)実行します。

たとえば、 1ノードあたり 4CPU コアをもつような並列計算機において、 2レプリカの計算を、1つあたり4並列のソルバーで実行する場合を考えます。
``mpiexec -np 2 abics input.toml`` として実行した場合、ノード 0 の最初の2コアでレプリカ制御プロセス A,B が起動し、それぞれが4並列ソルバー a,b を新規に起動します。
そのとき、ソルバー a はノード 0 の残り2コアとノード 1 の先頭 2コアに、
ソルバー b はノード 1 の残り2コアとノード 2 の先頭 2コアに配置され、ソルバー内でノード間通信が発生し、性能が低下します。

初期プロセスを多めに取ることで、プロセスを整列させ、不要なノードまたぎが起きないようにできます。
今回の例では、 ``mpiexec -np 4 abics input.toml`` とすることで、
レプリカ制御プロセス A,B の他に何もしないプロセスで ノード0 をすべて埋めることができ、
ソルバー a,b はそれぞれノード1, 2 を埋めることができます。


MPI 実装依存性
=========================
MPI 実装によっては、 ``MPI_Comm_spawn`` 関数で、「合計でいくつのプロセスを起動できるか」 という情報 (``MPI_UNIVERSE_SIZE``) を利用することがあります。
``MPI_UNIVERSE_SIZE`` の設定方法など、 各種MPI 実装に関するTips を紹介します。

OpenMPI
~~~~~~~~~~~~~
使用できるCPU コア数と同じ値が ``MPI_UNIVERSE_SIZE`` として自動で設定されます。
使用プロセス数がこれを超える場合には、 ``--oversubscribe`` オプションを渡す必要があります。

OpenMPI は、 ``MPI_Comm_spawn`` で呼び出したプロセスが非ゼロのリターンコードを返した場合に、すべてのプロセスごと終了します。
リターンコードを無視したい場合は、 ``--mca orte_abort_on_non_zero_status 0`` としてください。
例えばQuantum ESPRESSO は、計算中に浮動小数点数例外を捕捉すると、終了時に非ゼロのリターンコードを返します。

MPICH / Intel MPI
~~~~~~~~~~~~~~~~~~~~~
``-usize <num>`` オプションで ``MPI_UNIVERSE_SIZE`` を指定できます。
実際には指定しなくとも動作するようです。

HPE (SGI) MPT
~~~~~~~~~~~~~~~~~~~
``-up <num>`` オプションで ``MPI_UNIVERSE_SIZE`` を指定できます。
``-np <num>`` オプションよりも先に指定する必要があります。

その他
~~~~~~~~~~
大規模スパコンなどでは、ベンダーがジョブスケジューラとともに専用の MPI 実行スクリプトを用意している場合があります。
その場合は、個別のマニュアルを参考にしてください。

例として、 物性研スパコン sekirei, enaga では、 ``mpijob -spawn`` とすることで、 ``MPI_UNIVERSE_SIZE`` が自動的に設定されます。


abics の出力
------------------
各レプリカディレクトリ以下に計算結果が出力されます。

``structure.XXX.vasp``
=========================
各ステップごとの原子配置が VASP の POSCAR ファイル形式で出力されます。
ステップ番号がファイル名の ``XXX`` に入ります。

``minE.vasp``
====================
最低エネルギーを与えた原子位置が VASP の POSCAR ファイル形式で出力されます。

``obs.dat``
===================
各ステップごとの温度とエネルギーが電子ボルト単位で出力されます。

``obs_save.npy``
==================
各ステップごとのエネルギーが電子ボルト単位で出力されます。
``numpy.load('obs_save.npy')`` で、 ``darray`` として読み取ることができます。

``kT_hist.npy``
==================
各ステップごとの温度（電子ボルト単位）が Numpy バイナリ形式で出力されます。
``numpy.load('kT_hist.npy')`` で、 ``darray`` として読み取ることができます。

``Trank_hist.npy``
==================
各ステップごとの温度インデックスが Numpy バイナリ形式で出力されます。
``numpy.load('Trank_hist.npy')`` で、 ``darray`` として読み取ることができます。

